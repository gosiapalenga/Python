{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced Data set\n",
    "# https://www.youtube.com/watch?v=iCxtl44NMek&ab_channel=AnalyticswithAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48f820",
   "metadata": {},
   "source": [
    "# Hold Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd42fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation is a method to estimate the performance of a machine learning model.\n",
    "# It is a technique for evaluating a machine learning model and testing its performance.\n",
    "# We use training data set and split it into smaller ones.\n",
    "\n",
    "# Cross-Validation also referred to as out of sampling technique is an essential element of a data science project. \n",
    "# It is a resampling procedure used to evaluate machine learning models and access how the model will perform for \n",
    "# an independent test dataset.\n",
    "\n",
    "# While the primary significance of cross-validation is to validate a model and test its accuracy, there are more factors \n",
    "# that make this method important. \n",
    "\n",
    "# More than validating the model and testing its accuracy, cross-validation is used for measuring overfitting and any other \n",
    "# errors that might be noticed while testing the model. Overfitting refers to a concept wherein a statistical model works \n",
    "# against its training data set and does not show accuracy as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the main dataset in train and test sets. We do training on the training set, and validation on the train set.\n",
    "# Advantages: quick to execute\n",
    "# Disadvantage: not suitable for unbalanced dataset, not suitable for small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54118dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset is : 0.9619047619047619\n",
      "Accuracy on test dataset is : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "log = LogisticRegression()  \n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "log.fit(x_train,y_train)\n",
    "\n",
    "y_pred = log.predict(x_test)\n",
    "\n",
    "print(f\"Accuracy on training dataset is : {accuracy_score(y_train,log.predict(x_train))}\")\n",
    "print(f\"Accuracy on test dataset is : {accuracy_score(y_test,log.predict(x_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae929a7",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f1784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the whole dataset into k parts. \n",
    "# Call each partition as a fold\n",
    "# Use 1 fold for Validation\n",
    "# Use k-1 fold for training purpose\n",
    "# This technique is repeated k times, until each fold is used as a validation data, and remaining folds used as a training data.\n",
    "# The final accuracy of the model is computed by taking the mean accuracy of k models validation data\n",
    "# Pros: the entire dataset is is actually used as training set and validation set\n",
    "# Cons: it shouldn't be used on unbalanced dataset (eg only class 0 will be presented in the training data set, no class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53f5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e60240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\go27s\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\go27s\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation scores are: [1.         1.         0.86666667 0.93333333 0.83333333]\n",
      "Average Cross Validation scores is: 0.9266666666666665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\go27s\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\go27s\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "score = cross_val_score(log,X,y,cv=kf)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "score = cross_val_score(log,X,y,cv=kf)\n",
    "\n",
    "print(f\"Cross Validation scores are: {score}\")    # for each split\n",
    "print(f\"Average Cross Validation scores is: {score.mean()}\")   # mean of 5 splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b906c5",
   "metadata": {},
   "source": [
    "# Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707d8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for unbalanced dataset\n",
    "# Not suitable for Time Series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a240b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1 3] TEST: [0 2]\n",
      "TRAIN: [0 2] TEST: [1 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d1765",
   "metadata": {},
   "source": [
    "# Leave-p-out cross validation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a97255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p samples are used for validation set and the remaining N minus P samples are used as training set\n",
    "# If I have 100 samples in my dataset, I use 10 as p, then in each iteration 10 values will be used as validation dataset,\n",
    "# and the remaining 90 samples will be used as my training dataset.\n",
    "# This process is repeated till the whole data set gets divided on the validation set of P samples and \n",
    "# N minus P training samples\n",
    "# Pros: all the data samples get used, both training and the validation samples\n",
    "# Cons: hihg computation time and not suitable for imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2669ae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeavePOut(p=2)\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [1 3] TEST: [0 2]\n",
      "TRAIN: [1 2] TEST: [0 3]\n",
      "TRAIN: [0 3] TEST: [1 2]\n",
      "TRAIN: [0 2] TEST: [1 3]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "lpo = LeavePOut(2)\n",
    "\n",
    "print (lpo)\n",
    "\n",
    "for train_index, test_index in lpo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829495c7",
   "metadata": {},
   "source": [
    "# Leave one out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afdc915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut()\n",
      "TRAIN: [1] TEST: [0]\n",
      "TRAIN: [0] TEST: [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([1, 2])\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "print(loo)\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69adc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
